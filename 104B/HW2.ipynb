{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header>\n",
    "    <center>    \n",
    "        <h1>Math 104B Homework 2</h1>\n",
    "        <h2>Rad Mallari, 8360828</h2>\n",
    "        <h2>April 12th, 2022</h2>\n",
    "    </center>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.)</b> Let $f(x)=e^{x}$<br>\n",
    "&emsp;<b>(a)</b> Compute the centered difference approximation of $f'(\\frac{1}{2})$, i.e. $D_{h}^{0}f(\\frac{1}{2})$, for $h=\\frac{0.1}{2^{n}}$, $n=0,1,\\dots,10$\n",
    "and verify its quadratic rate of convergence.<br>\n",
    "&emsp;<b>(b)</b> Determine approximately the optimal value of $h_{0}$ which gives the minimum total error (the sum of discretization error plus the round-off error)<br>\n",
    "&emsp;&emsp; and verify this numerically.<br>\n",
    "&emsp;<b>(c)</b> Construct and compute a fourth order approximation to $f'(\\frac{1}{2})$ by applying Richardson's extrapolation to $D_{h}^{0}f(\\frac{1}{2})$.<br>\n",
    "&emsp;&emsp; Verify the rate of convergence numerically.<br>\n",
    "&emsp;&emsp; What is the optimal $h_{0}$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Proof:</b><br>\n",
    "&emsp;&emsp;<b>(a)</b> Centered difference formula is defined as\n",
    "$$D_{h}^{0}f(x_{0})=\\frac{f(x_{0}+h)-f(x_{0}-h)}{2h}$$\n",
    "&emsp;&emsp;Therefore, $D_{h}^{0}f(\\frac{1}{2})$ for $h=\\frac{0.1}{2^{n}}$, $n=0,1,\\dots,10$ is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At h=0.1, the derivative of f(x) at x=1/2 is: 1.6514705137461927, it's error is: 0.002749243046064498\n",
      "At h=0.05, the derivative of f(x) at x=1/2 is: 1.6494083237722656, it's error is: 0.0006870530721374557\n",
      "At h=0.025, the derivative of f(x) at x=1/2 is: 1.6488930178661754, it's error is: 0.00017174716604717588\n",
      "At h=0.0125, the derivative of f(x) at x=1/2 is: 1.6487642064853159, it's error is: 4.293578518765884e-05\n",
      "At h=0.00625, the derivative of f(x) at x=1/2 is: 1.6487320045835219, it's error is: 1.0733883393676535e-05\n",
      "At h=0.003125, the derivative of f(x) at x=1/2 is: 1.6487239541670462, it's error is: 2.683466918007582e-06\n",
      "At h=0.0015625, the derivative of f(x) at x=1/2 is: 1.6487219415666488, it's error is: 6.708665205579223e-07\n",
      "At h=0.00078125, the derivative of f(x) at x=1/2 is: 1.6487214384167714, it's error is: 1.6771664324011226e-07\n",
      "At h=0.000390625, the derivative of f(x) at x=1/2 is: 1.6487213126291067, it's error is: 4.192897851140742e-08\n",
      "At h=0.0001953125, the derivative of f(x) at x=1/2 is: 1.6487212811830432, it's error is: 1.0482914980514124e-08\n",
      "Least h_0 error: 0.0001953125\n"
     ]
    }
   ],
   "source": [
    "from math import e\n",
    "f = lambda x : e**x\n",
    "x_0 = 1/2\n",
    "error_list = {}\n",
    "for n in range(10):\n",
    "    h=0.1/(2**n)\n",
    "    df_x = (f(x_0+h)-f(x_0-h))/(2*h)\n",
    "    print(f\"At h={h}, the derivative of f(x) at x=1/2 is: {df_x}, it's error is: {abs(f(1/2)-df_x)}\")\n",
    "    error_list[h] = abs(f(1/2)-df_x)\n",
    "min_error = min(error_list, key=error_list.get)\n",
    "print(f\"Least h error: {min_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<b>(b)</b> The most optimal value for $h_{0}$ is $0.0001953125$ since this returns the most minimal total error.<br>\n",
    "&emsp;&emsp;<b>(c)</b> Richardson's extrapolation of order four is:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    D_{h}^{ext}f(x_{0})&=\\frac{4D_{\\frac{h}{2}}^{0}f(x_{0})-D_{h}^{0}f(x_{0})}{3}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "&emsp;&emsp; Using this, we compute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At h=0.1, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.64872092711429, it's error is: 3.4358583822502453e-07\n",
      "At h=0.05, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.648721249230812, it's error is: 2.1469316102695757e-08\n",
      "At h=0.025, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.6487212693583626, it's error is: 1.341765587525856e-09\n",
      "At h=0.0125, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.6487212706162573, it's error is: 8.387091021688775e-11\n",
      "At h=0.00625, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.6487212706948877, it's error is: 5.240474720835664e-12\n",
      "At h=0.003125, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.6487212706998495, it's error is: 2.786659791809143e-13\n",
      "At h=0.0015625, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.6487212707001457, it's error is: 1.7541523789077473e-14\n",
      "At h=0.00078125, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.648721270699885, it's error is: 2.431388423929093e-13\n",
      "At h=0.000390625, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.648721270701022, it's error is: 8.93729534823251e-13\n",
      "At h=0.0001953125, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: 1.6487212707011167, it's error is: 9.885425811262394e-13\n",
      "Least h error: 0.0015625\n"
     ]
    }
   ],
   "source": [
    "error_list.clear()\n",
    "for n in range(10):\n",
    "    h=0.1/(2**n)\n",
    "    df_x_h_half = (f(x_0+(h/2))-f(x_0-(h/2)))/(2*(h/2))\n",
    "    df_x = (f(x_0+h)-f(x_0-h))/(2*h)\n",
    "    df_R_ext = ((4*df_x_h_half) - df_x)/3\n",
    "    print(f\"At h={h}, the derivative of f(x) using 4th order Richardson's extrapolation at x=1/2 is: {df_R_ext}, it's error is: {abs(f(1/2)-df_R_ext)}\")\n",
    "    error_list[h] = abs(f(1/2)-df_R_ext)\n",
    "\n",
    "min_error = min(error_list, key=error_list.get)\n",
    "print(f\"Least h error: {min_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; The most optimal $h_{0}$ in this case is $0.0015625$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.)</b> Use Taylor expansions to derive the error term of the sided difference approximation to $f'(x_{0})$:\n",
    "$$D_{h}f(x_{0})=\\frac{-f(x_{0}+2h)+4f(x_{0}+h)-3f(x_{0})}{2h}$$\n",
    "&emsp;<b>Proof:</b><br>\n",
    "&emsp;&emsp; By the notes, we know that the Taylor series expansion of $f(x_{0}+h)$ is:\n",
    "$$f(x_{0}+h)=f(x_{0})+f'(x_{0})h+\\frac{1}{2!}f''(x_{0})h^{2}+\\frac{1}{3!}f^{(3)}(x_{0})h^{3}+...$$\n",
    "&emsp;&emsp; This implies that $f(x_{0}+2h)$ is given by:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "f(x_{0}+2h)&=f(x_{0})+f'(x_{0})(2h)+\\frac{1}{2!}f''(x_{0})(2h)^{2}+\\frac{1}{3!}f^{(3)}(x_{0})(2h)^{3}+...\\\\\n",
    "&=f(x_{0})+f'(x_{0})2h+\\frac{1}{2!}f''(x_{0})4h^{2}+\\frac{1}{3!}f^{(3)}(x_{0})8h^{3}+...\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "&emsp;&emsp; Using this, we know $D_{h}f(x_{0})$ is:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "D_{h}f(x_{0})&=\\frac{-f(x_{0})-2f'(x_{0})h-\\frac{4}{2!}f''(x_{0})h^{2}-\\frac{8}{3!}f^{(3)}(x_{0})h^{3}-...+4f(x_{0})+4f'(x_{0})h+\\frac{4}{2!}f''(x_{0})h^{2}+\\frac{4}{3!}f^{(3)}(x_{0})h^{3}+...-3f(x_{0})}{2h}\\\\\n",
    "&=\\frac{2f'(x_{0})h-\\frac{4}{3!}f^{(3)}(x_{0})h^{3}-...}{2h}\\\\\n",
    "&=f'(x_{0})-\\frac{2}{3!}f^{(3)}(x_{0})h^{2}-...\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "&emsp;&emsp; Since the error is $|f'(x_{0})-D_{h}f(x_{0})|$, the error term is $\\frac{2}{3!}f^{(3)}(x_{0})h^{2}-...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.)</b> Consider the data points $(x_{0},f_{0}),(x_{1},f_{1}),\\dots,(x_{n},f_{n})$, where the points $x_{0},x_{1},\\dots,x_{n}$ are distinct but otherwise arbitrary (they could, for example, be Chebyshev nodes).<br>\n",
    "&emsp;&emsp;Then the derivative of the interpolating polynomial of these data is:\n",
    "$$P_{n}'(x)=\\sum_{j=0}^{n}l_{j}'(x)f_{j},$$\n",
    "&emsp;&emsp;where the $l_{j}$'s are the elementary Lagrange polynomials:\n",
    "$$l_{j}(x)=\\frac{1}{\\alpha_{j}}\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x-x_{k}),\\quad \\alpha_{j}=\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x_{j}-x_{k})$$\n",
    "&emsp;&emsp;We can evaluate the first equation at each nodes $x_{0},x_{1},\\dots,x_{n}$ which will give us an approximation to the derivative of $f$ at those points, i.e. $f'(x_{i})\\approx P_{n}'(x_{i})$. We can write this as\n",
    "$$\\textbf{f}'\\approx D_{n}\\textbf{f}$$\n",
    "&emsp;&emsp;where $\\textbf{f}=[f_{0}f_{1}\\dots f_{n}]^{T}$, $\\textbf{f}'=[f(x_{0})f(x_{1})\\dots f'(x_{n})]^{T}$ and $D_{n}$ is the $\\textbf{Differentiation Matrix}$, $(D_{n})_{ij}=l_{j}'(x_{i})$.<br>\n",
    "&emsp;&emsp;<b>(a)</b> Prove that:\n",
    "$$l_{j}'(x)=l_{j}(x)\\sum_{\\substack{k=0\\\\k\\neq j}}^{n}\\frac{1}{x-x_{k}}$$\n",
    "&emsp;&emsp;Hint: differentiate $\\log l_{j}(x)$.<br>\n",
    "&emsp;&emsp;<b>(b)</b> Using the equation above, prove that\n",
    "$$D(n_{n})_{ij}=\\frac{\\alpha_{i}}{\\alpha_{j}}\\left(\\frac{1}{x_{i}-x_{j}}\\right),\\quad i\\neq j$$\n",
    "$$D(n_{n})_{ii}=\\sum_{\\substack{k=0\\\\k\\neq i}}^{n}\\frac{1}{x-x_{k}}$$\n",
    "&emsp;&emsp;<b>(c)</b> Prove that:\n",
    "$$\\sum_{j=0}^{n}(D_{n})_{ij}=0\\quad\\text{for all }i=0,1,\\dots,n$$\n",
    "&emsp;&emsp;<b>(d)</b> Obtain $D_{2}$ for the Chebyshev points $x_{0}=-1$, $x_{1}=0$, $x_{2}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Proof:</b><br>\n",
    "&emsp;&emsp;<b>(a)</b> Per the hint, taking the the $\\log$ of $l_{j}(x)$ yields:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\log l_{j}(x)&=\\log\\left(\\frac{\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x-x_{k})}{\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x_{j}-x_{k})}\\right)\\\\\n",
    "    &=\\log\\left(\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x-x_{k})\\right) - \\log\\left(\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x_{j}-x_{k})\\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "&emsp;&emsp; Then differentiating with respect to $x$ gives us:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "    \\frac{l_{j}'(x)}{l_{j}(x)}&=\\frac{1}{x-x_{0}}+\\frac{1}{x-x_{1}}+...+\\frac{1}{x-x_{j-1}}+\\frac{1}{x-x_{j+1}}+...+\\frac{1}{x-x_{n}}\\\\\n",
    "    &=\\sum_{\\substack{k=0\\\\k\\neq j}}^{n}\\frac{1}{x-x_{k}}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "&emsp;&emsp; Finally, multiplying by $l_{j}(x)$ to both sides:\n",
    "$$l_{j}'(x)=l_{j}(x)\\sum_{\\substack{k=0\\\\k\\neq j}}^{n}\\frac{1}{x-x_{k}}$$\n",
    "&emsp;&emsp;<b>(b)</b> We are given that $(D_{n})_{ij}=l_{j}'(x_{i})$, where $D_{n}$ is the Differentiation Matrix.<br>\n",
    "&emsp;&emsp; Therefore, equating the given with the result from <b>(a)</b>:\n",
    "$$(D_{n})_{ij}=l_{j}(x_{i})\\sum_{\\substack{k=0\\\\k\\neq j}}^{n}\\frac{1}{x_{i}-x_{k}}$$\n",
    "&emsp;&emsp; Furthermore, we know\n",
    "$$l_{j}(x_{i})=\\frac{\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x_{i}-x_{k})}{\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x_{j}-x_{k})}=\\frac{\\alpha_{i}}{\\alpha_{j}},\\quad i\\neq j$$\n",
    "&emsp;&emsp; Using this, $(D_{n})_{ij}$ becomes:\n",
    "$$(D_{n})_{ij}=\\frac{\\alpha_{i}}{\\alpha_{j}}\\left(\\sum_{\\substack{k=0\\\\k\\neq j}}^{n}\\frac{1}{x_{i}-x_{k}}\\right)=\\frac{\\alpha_{i}}{\\alpha_{j}}\\left(\\frac{1}{x_{i}-x_{j}}\\right),\\quad i\\neq j$$\n",
    "&emsp;&emsp; And if $i=j$, $l_{j}(x_{i})=\\frac{\\alpha_{i}}{\\alpha_{i}}=1$, so $(D_{n})_{ii}$ is simply:\n",
    "$$(D_{n})_{ii}=\\sum_{\\substack{k=0\\\\k\\neq i}}^{n}\\frac{1}{x_{i}-x_{k}}$$\n",
    "&emsp;&emsp;<b>(c)</b> $\\sum_{j=0}^{n}(D_{n})_{ij}=0\\quad\\text{for all }i=0,1,\\dots,n$ is given as:\n",
    "$$(D_{n})_{i0}+(D_{n})_{i1}+(D_{n})_{i2}+...=$$\n",
    "&emsp;&emsp;<b>(d)</b> We can find this using \n",
    "$$(D_{2})_{ij}=\\frac{\\alpha_{i}}{\\alpha_{j}}\\left(\\frac{1}{x_{i}-x_{j}}\\right),\\quad i\\neq j$$\n",
    "&emsp;&emsp;and\n",
    "$$(D_{2})_{ii}=\\sum_{\\substack{k=0\\\\k\\neq i}}^{n}\\frac{1}{x_{i}-x_{k}},\\quad\\text{otherwise}$$\n",
    "&emsp;&emsp; Now computing each entry of the Differentiation Matrix:\n",
    "$$(D_{2})_{00}=\\sum_{\\substack{k=0\\\\k\\neq i}}^{2}\\frac{1}{x_{i}-x_{k}}=-\\frac{3}{2}$$\n",
    "$$(D_{2})_{01}=2, (D_{2})_{02}=-\\frac{1}{2},(D_{2})_{10}=-\\frac{1}{2},(D_{2})_{11}=0,(D_{2})_{12}=\\frac{1}{2},(D_{2})_{20}=\\frac{1}{2},(D_{2})_{21}=-2,(D_{2})_{22}=\\frac{3}{2}$$\n",
    "&emsp;&emsp; And so our differentiation matrix $D_{2}$ is:\n",
    "$$D_{2}=\\begin{bmatrix}\n",
    "-\\frac{3}{2}&2&-\\frac{1}{2}\\\\\n",
    "-\\frac{1}{2}&0&\\frac{1}{2}\\\\\n",
    "\\frac{1}{2}&-2&\\frac{3}{2}\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183e1a4208f4ff2349d14631aeed6337dfdb045e2deedee598ad16d6076c51b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
