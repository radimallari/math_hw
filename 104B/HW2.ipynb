{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<header>\n",
    "    <center>    \n",
    "        <h1>Math 104B Homework 2</h1>\n",
    "        <h2>Rad Mallari, 8360828</h2>\n",
    "        <h2>April 12th, 2022</h2>\n",
    "    </center>\n",
    "</header>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1.)</b> Let $f(x)=e^{x}$<br>\n",
    "&emsp;<b>(a)</b> Compute the centered difference approximation of $f'(\\frac{1}{2})$, i.e. $D_{h}^{0}f(\\frac{1}{2})$, for $h=\\frac{0.1}{2^{n}}$, $n=0,1,\\dots,10$\n",
    "and verify its quadratic rate of convergence.<br>\n",
    "&emsp;<b>(b)</b> Determine approximately the optimal value of $h_{0}$ which gives the minimum total error (the sum of discretization error plus the round-off error) and verify this numerically.<br>\n",
    "&emsp;<b>(c)</b> Construct and compute a fourth order approximation to $f'(\\frac{1}{2})$ by applying Richardson's extrapolation to $D_{h}^{0}f(\\frac{1}{2})$.<br>\n",
    "&emsp;&emsp;Verify the rate of convergence numerically.<br>\n",
    "&emsp;&emsp;What is the optimal $h_{0}$ in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;<b>Proof:</b><br>\n",
    "&emsp;&emsp;<b>(a)</b> Centered difference formula is defined as\n",
    "$$D_{h}^{0}f(x_{0})=\\frac{f(x_{0}+h)-f(x_{0}-h)}{2h}$$\n",
    "&emsp;&emsp;Therefore, $D_{h}^{0}f(\\frac{1}{2})$ for $h=\\frac{0.1}{2^{n}}$, $n=0,1,\\dots,10$ is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At h=0.1, the derivative is: 1.6514705137461927\n",
      "At h=0.05, the derivative is: 1.6494083237722656\n",
      "At h=0.025, the derivative is: 1.6488930178661754\n",
      "At h=0.0125, the derivative is: 1.6487642064853159\n",
      "At h=0.00625, the derivative is: 1.6487320045835219\n",
      "At h=0.003125, the derivative is: 1.6487239541670462\n",
      "At h=0.0015625, the derivative is: 1.6487219415666488\n",
      "At h=0.00078125, the derivative is: 1.6487214384167714\n",
      "At h=0.000390625, the derivative is: 1.6487213126291067\n",
      "At h=0.0001953125, the derivative is: 1.6487212811830432\n"
     ]
    }
   ],
   "source": [
    "from math import e\n",
    "f = lambda x : e**x\n",
    "x = 1/2\n",
    "for n in range(10):\n",
    "    h=0.1/(2**n)\n",
    "    df_x = (f(x+h)-f(x-h))/(2*h)\n",
    "    print(f\"At h={h}, the derivative is: {df_x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.)</b> Use Taylor expansions to derive the error term of the sided difference approximation to $f'(x_{0})$:\n",
    "$$D_{h}f(x_{0})=\\frac{-f(x_{0}+2h)+4f(x_{0}+h)-3f(x_{0})}{2h}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3.)</b> Consider the data points $(x_{0},f_{0}),(x_{1},f_{1}),\\dots,(x_{n},f_{n})$, where the points $x_{0},x_{1},\\dots,x_{n}$ are distinct but otherwise arbitrary (they could, for example, be Chebyshev nodes).<br>\n",
    "&emsp;Then the derivative of the interpolating polynomial of these data is:\n",
    "$$P_{n}'(x)=\\sum_{j=0}^{n}l_{j}'(x)f_{j},$$\n",
    "&emsp;where the $l_{j}$'s are the elementary Lagrange polynomials:\n",
    "$$l_{j}(x)=\\frac{1}{\\alpha_{j}}\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x-x_{k}),\\quad \\alpha_{j}=\\prod_{\\substack{k=0\\\\k\\neq j}}^{n}(x_{j}-x_{k})$$\n",
    "&emsp;We can evaluate the first equation at each nodes $x_{0},x_{1},\\dots,x_{n}$ which will give us an approximation to the derivative of $f$ at those points, i.e. $f'(x_{i})\\approx P_{n}'(x_{i})$. We can write this as\n",
    "$$\\textbf{f}'\\approx D_{n}\\textbf{f}$$\n",
    "&emsp;where $\\textbf{f}=[f_{0}f_{1}\\dots f_{n}]^{T}$, $\\textbf{f}'=[f(x_{0})f(x_{1})\\dots f'(x_{n})]^{T}$ and $F_{n}$ is the $\\textbf{Differentiation Matrix}$, $(D_{n})_{ij}=l_{j}'(x_{i})$.<br>\n",
    "&emsp;&emsp;<b>(a)</b> Prove that:\n",
    "$$l_{j}'(x)=l_{j}(x)\\sum_{\\substack{k=0\\\\k\\neq j}}^{n}\\frac{1}{x-x_{k}}$$\n",
    "&emsp;&emsp;Hint: differentiate $\\log l_{j}(x)$.<br>\n",
    "&emsp;&emsp;<b>(b)</b> Using the equation above, prove that\n",
    "$$D(n_{n})_{ij}=\\frac{\\alpha_{i}}{\\alpha_{j}}\\left(\\frac{1}{x_{i}-x_{j}}\\right),\\quad i\\neq j$$\n",
    "$$D(n_{n})_{ii}=\\sum_{\\substack{k=0\\\\k\\neq i}}^{n}\\frac{1}{x-x_{k}}$$\n",
    "&emsp;&emsp;<b>(c)</b> Prove that:\n",
    "$$\\sum_{j=0}^{n}(D_{n})_{ij}=0\\quad\\text{for all }i=0,1,\\dots,n$$\n",
    "&emsp;&emsp;<b>(d)</b> Obtain $D_{2}$ for the Chebyshev points $x_{0}=-1$, $x_{1}=0$, $x_{2}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "183e1a4208f4ff2349d14631aeed6337dfdb045e2deedee598ad16d6076c51b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
